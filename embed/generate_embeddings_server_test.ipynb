{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenSearch client\n",
    "client = OpenSearch(\n",
    "    hosts=[{\"host\": \"opensearch-ds.ifi.uni-heidelberg.de\", \"port\": 443}],\n",
    "    http_auth=(\"asiddhpura\", \"Pkw?#Rivale9Meran.Abweg\"),\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=60,\n",
    ")\n",
    "\n",
    "# Create a Point in Time\n",
    "pit = client.create_point_in_time(index=\"frameintell_arxiv_metadata\", keep_alive=\"5m\")\n",
    "pit_id = pit[\"pit_id\"]\n",
    "# print(f\"Point in Time ID: {pit_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get oldest paper\n",
    "query = {\n",
    "    \"size\": 1,\n",
    "    \"_source\": [\"update_date\"],  # We only need the update_date field\n",
    "    \"query\": {\"match_all\": {}},  # Match all documents\n",
    "    \"sort\": [{\"update_date\": {\"order\": \"asc\"}}],  # Sort by update_date in ascending order\n",
    "}\n",
    "\n",
    "# Get the end update_date from the first document\n",
    "response = client.search(index=\"frameintell_arxiv_metadata\", body=query)\n",
    "end_date = response[\"hits\"][\"hits\"][0][\"_source\"][\"update_date\"]\n",
    "print(f\"Oldest paper update_date: {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary update_date\n",
    "# end_date = \"2024-05-10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the start date for search_after\n",
    "# end_date = datetime.strptime(update_date, \"%Y-%m-%d\")\n",
    "# end_date = end_date.strftime(\"%Y-%m-%d\")\n",
    "start_date = \"now\"\n",
    "\n",
    "# Initial search query\n",
    "query = {\n",
    "    \"size\": 100,\n",
    "    \"_source\": [\"id\", \"abstract\", \"update_date\"],\n",
    "    \"query\": {\"range\": {\"update_date\": {\"lte\": start_date, \"gte\": end_date}}},\n",
    "    \"sort\": [{\"update_date\": {\"order\": \"desc\"}}, {\"_id\": \"desc\"}],  # tie breaker\n",
    "    \"pit\": {\"id\": pit_id, \"keep_alive\": \"5m\"},\n",
    "}\n",
    "\n",
    "# Perform the initial search\n",
    "response = client.search(body=query)\n",
    "results = response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping for the target index\n",
    "mappings = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"abstract\": {\"type\": \"text\"},\n",
    "            \"processed_abstract\": {\"type\": \"text\"},\n",
    "            \"embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 384,\n",
    "            },\n",
    "            \"update_date\": {\"type\": \"date\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Create the target index to store the embeddings\n",
    "target_index = \"frameintell_arxiv_embeddings\"\n",
    "if not client.indices.exists(index=target_index):\n",
    "    client.indices.create(index=target_index, body=mappings)\n",
    "    print(f\"Index {target_index} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the set of English stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "# Process the abstracts\n",
    "def process_abstracts(abstracts):\n",
    "    processed_abstracts = []\n",
    "    for abstract in abstracts:\n",
    "        if not isinstance(abstract, str):\n",
    "            processed_abstracts.append(\"\")\n",
    "            continue\n",
    "        abstract = abstract.replace(\"\\n\", \" \")  # Remove newline characters\n",
    "        abstract = abstract.lower()  # Convert to lowercase\n",
    "        abstract = re.sub(r\"[^a-zA-Z\\s]\", \"\", abstract)  # Remove special characters and digits\n",
    "        words = abstract.split()  # Tokenize the abstract\n",
    "        processed_abstract = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "        processed_abstract = \" \".join(processed_abstract)  # Combine the processed text as a string\n",
    "        processed_abstracts.append(processed_abstract)\n",
    "    return processed_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_documents = 0\n",
    "# Continue fetching documents in batches until there are no more documents\n",
    "while True:\n",
    "    # If there are no more documents, break the loop\n",
    "    if not results:\n",
    "        break\n",
    "\n",
    "    # Get the abstracts and process them\n",
    "    abstracts = [result[\"_source\"][\"abstract\"] for result in results]\n",
    "    processed_abstracts = process_abstracts(abstracts)\n",
    "\n",
    "    # Get the embeddings\n",
    "    embeddings = model.encode(processed_abstracts).tolist()\n",
    "\n",
    "    # Prepare the documents to be indexed\n",
    "    bulk_data = []\n",
    "    for i, result in enumerate(results):\n",
    "        doc = {\n",
    "            \"id\": result[\"_source\"][\"id\"],\n",
    "            \"abstract\": result[\"_source\"][\"abstract\"],\n",
    "            \"processed_abstract\": processed_abstracts[i],\n",
    "            \"embedding\": embeddings[i],\n",
    "            \"update_date\": result[\"_source\"][\"update_date\"],\n",
    "        }\n",
    "        bulk_data.append({\"index\": {\"_index\": target_index, \"_id\": doc[\"id\"]}})\n",
    "        bulk_data.append(doc)\n",
    "\n",
    "    # Index the documents\n",
    "    client.bulk(body=bulk_data)\n",
    "\n",
    "    # Increment the total documents counter\n",
    "    total_documents += len(results)\n",
    "    print(total_documents)\n",
    "    # Get and print the last update_date that was indexed\n",
    "    last_update_date = results[-1][\"_source\"][\"update_date\"]\n",
    "    print(f\"Last update_date: {last_update_date}\")\n",
    "\n",
    "    # Get the last sort value\n",
    "    last_sort = results[-1][\"sort\"]\n",
    "    query[\"search_after\"] = last_sort\n",
    "    response = client.search(body=query)\n",
    "    results = response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the Point in Time and release resources\n",
    "client.delete_point_in_time(body={\"pit_id\": pit_id})\n",
    "client.transport.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
